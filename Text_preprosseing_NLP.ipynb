{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFN6Mq9oMoDbgfu+yeLxwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahuldahiya67/NLP/blob/main/Text_preprosseing_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "9gGxGr3QMfjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Rahuldahiya67/NLP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrLRpsudTIST",
        "outputId": "11da9cb3-c393-4852-b9c2-58d257504a99"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), 788 bytes | 788.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "b7irCmZxNrEM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords and wordnet from NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geVxcjiRN1H7",
        "outputId": "1de53cd7-3aca-4a70-a9fa-358e9d2957a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define text to be preprocessed\n",
        "text = \"This is a sample text! It contains punctuations, stopwords, and needs to be preprocessed.\"\n"
      ],
      "metadata": {
        "id": "tycoNkvRN1z4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "text = text.lower()\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE55wRnoN5Jl",
        "outputId": "37a5b84b-a814-4ff3-d471-bfe6a6f7e80f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a sample text it contains punctuations stopwords and needs to be preprocessed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers and special characters\n",
        "text = re.sub(r'\\d+', '', text) # Remove numbers\n",
        "text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuations\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjEYsRSyN73n",
        "outputId": "a73a571f-9362-4795-c0ec-f4a15f4f2ea2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a sample text it contains punctuations stopwords and needs to be preprocessed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1QC2_Y-N-lA",
        "outputId": "720d1e34-4c60-4900-91bc-274547123c22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'sample', 'text', 'it', 'contains', 'punctuations', 'stopwords', 'and', 'needs', 'to', 'be', 'preprocessed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(stop_words)\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBvukdSxOBX2",
        "outputId": "34da1320-a60b-490a-bb5a-642f9ccbacc6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'yourselves', 'does', 'been', 'once', \"wasn't\", 'few', 'can', 'why', 'down', \"don't\", 'you', 'our', 'who', 'own', 'very', 'shan', 'doesn', 'through', 'nor', 'are', 'm', 'hers', 'only', 'its', 'd', 're', 'at', 'same', 'needn', \"wouldn't\", 'themselves', \"weren't\", 'her', 'when', 'this', 'to', 'from', 'up', 'do', 'off', 'the', \"hasn't\", 'for', 'more', 'those', 'didn', 'by', 'during', 'them', 'shouldn', 'over', 'about', 'what', 'mustn', \"mightn't\", 'himself', 'having', 'will', 've', 'wouldn', 'these', 'of', 'myself', 'where', 'no', 'as', 'mightn', 'again', 'wasn', 'ain', \"that'll\", 'before', 'your', 'whom', \"mustn't\", 's', 'll', 'ourselves', 'theirs', 'or', 'but', 'further', 'isn', 'any', 'yourself', 'had', 'ours', \"needn't\", 'most', 'him', 'each', 'an', 'be', 'against', 'haven', 'both', 'below', \"doesn't\", 'which', \"you'd\", 'was', 'we', 'y', 'too', 'am', \"isn't\", 'don', 'she', \"it's\", 'o', 'then', 'they', 'how', 'he', 'on', 'i', \"you've\", 'not', 'into', 'me', 'were', 'just', 'after', 'such', 'should', \"she's\", 'that', 'and', 'while', 'it', \"couldn't\", 'if', 'now', 'above', 'other', \"you're\", 'there', \"won't\", 'a', 'in', 'have', 'so', \"should've\", 'until', 'ma', 'has', \"aren't\", 'hasn', 'aren', 'itself', 'between', 'some', \"hadn't\", \"shan't\", 'yours', \"you'll\", 'did', \"haven't\", 'their', 'than', 'my', 'herself', 'hadn', 'out', 'his', 'is', 'here', 't', 'weren', 'under', 'doing', \"shouldn't\", 'being', 'couldn', 'won', 'because', \"didn't\", 'with', 'all'}\n",
            "['sample', 'text', 'contains', 'punctuations', 'stopwords', 'needs', 'preprocessed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize tokens\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV6WOqraONje",
        "outputId": "8811ac17-6d28-4831-b683-9792ec3b0871"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'text', 'contains', 'punctuation', 'stopwords', 'need', 'preprocessed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print preprocessed text\n",
        "preprocessed_text = \" \".join(lemmatized_tokens)\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0Z10M0lORs8",
        "outputId": "10ae1d2a-bf6d-4647-c9ce-651906352cd3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample text contains punctuation stopwords need preprocessed\n"
          ]
        }
      ]
    }
  ]
}